{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_oneHotLabels(targets,nClasses):\n",
    "    oneHotLabels = np.zeros((len(targets),nClasses))\n",
    "    for i in range(len(targets)):\n",
    "        oneHotLabels[i][targets[i]] = 1.0\n",
    "    return oneHotLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 150\n"
     ]
    }
   ],
   "source": [
    "nClasses = len(iris.target_names)\n",
    "nFeatures = len(iris.feature_names)\n",
    "data,targets = MinMaxScaler().fit_transform(iris.data),to_oneHotLabels(iris.target,nClasses)\n",
    "print(\"Total Samples: %d\"%(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 150 | Testing Samples: 105\n"
     ]
    }
   ],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(data,targets,test_size=0.3,shuffle=True)\n",
    "print(\"Training Samples: {} | Testing Samples: {}\".format(len(data),xtrain.shape[0],xtest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self,params,inputSize,learning_rate=1.0,learning_rate_decay=0.99,relu_alpha=0.0):\n",
    "        self.model = self.create_neuralNetwork(params,inputSize)\n",
    "    \n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.relu_alpha = relu_alpha\n",
    "        \n",
    "    def create_neuralNetwork(self,params,inputSize):\n",
    "        model = []\n",
    "        nNeurons = 0\n",
    "        for param in params:\n",
    "            model.append([np.random.normal(size=(inputSize,param)), np.zeros((1,param))])\n",
    "            inputSize = param\n",
    "        print(\"Created a neural network with %d hidden layers and %d neurons \"%(len(model)-1,sum(params)))\n",
    "        return model\n",
    "    \n",
    "    def predict(self,xInput):\n",
    "        xInput = np.expand_dims(xInput,axis=0)\n",
    "        for weights,bias in self.model[:-1]:\n",
    "            xInput = self.relu(np.dot(xInput,weights) + bias)\n",
    "        return self.sigmoid(np.dot(xInput,self.model[-1][0])+ self.model[-1][1])\n",
    "    \n",
    "    def compute(self,xInput):\n",
    "        # z = weights * xInput + bias or z = w*x + b\n",
    "        zValues = []\n",
    "        \n",
    "        # a is either sigmoid(z) or # relu(z)\n",
    "        aValues = [] # holds activated z values by sigmoid or relu functions\n",
    "        \n",
    "        xInput = np.expand_dims(xInput,axis=0)\n",
    "        for weights,bias in self.model[:-1]:\n",
    "            z = np.dot(xInput,weights) + bias\n",
    "            zValues.append(z)\n",
    "            \n",
    "            a = self.relu(z)\n",
    "            aValues.append(a)\n",
    "            xInput = a\n",
    "            \n",
    "        z = np.dot(xInput,self.model[-1][0])+self.model[-1][1]\n",
    "        zValues.append(z)\n",
    "        aValues.append(self.sigmoid(z))\n",
    "\n",
    "        return aValues,zValues\n",
    "    \n",
    "    def predict_batch(self,batch,final_prediction=False):\n",
    "        \n",
    "        if final_prediction:\n",
    "            predictions = []\n",
    "            for x in batch:\n",
    "                predictions.append(self.predict(x))\n",
    "            return predictions\n",
    "        \n",
    "        else:\n",
    "            batchValues = {\"a\":[],\"z\":[]}\n",
    "            for x in batch:\n",
    "                aValues,zValues = self.compute(x)\n",
    "                batchValues[\"a\"].append(aValues)\n",
    "                batchValues[\"z\"].append(zValues)\n",
    "                \n",
    "            return batchValues\n",
    "            \n",
    "            \n",
    "        \n",
    "    def relu(self,z,deriv=False):\n",
    "        if deriv:\n",
    "            z[z<0] = 0 \n",
    "            z[z>0] = 1\n",
    "            return z\n",
    "        return np.maximum(z,self.relu_alpha)\n",
    "\n",
    "    def sigmoid(self,z,deriv=False):\n",
    "        out = 1/(1+np.exp(-z))\n",
    "        if deriv:\n",
    "            return out*(1-out)\n",
    "        return out\n",
    "    \n",
    "    def backprop(self,batchValues,xtrain,ytrain):\n",
    "         # delta  = dCost / dWeights = dC/dW = dC/dA * dA/dZ * dZ/dW\n",
    "        delta = [[np.zeros(weights.shape),np.zeros(bias.shape)] for weights,bias in self.model]\n",
    "        \n",
    "        cost = 0\n",
    "        for i in range(len(xtrain)):\n",
    "            iw = len(self.model)\n",
    "            err = 0\n",
    "            for _ in range(iw):\n",
    "                iw -= 1\n",
    "                \n",
    "                if iw != 0:\n",
    "                    layer_xInput = batchValues[\"a\"][i][iw-1]\n",
    "                else:\n",
    "                    layer_xInput = xtrain[i].reshape(1,4)\n",
    "                    \n",
    "                layer_zValue = batchValues[\"z\"][i][iw]\n",
    "                layer_aValue = batchValues[\"a\"][i][iw]\n",
    "                \n",
    "                if iw == len(self.model)-1:\n",
    "                    cost += self.mean_squared_error(layer_aValue,ytrain[i])\n",
    "                    err = self.mean_squared_error(layer_aValue,ytrain[i],True)*self.sigmoid(layer_zValue,True)\n",
    "                else:\n",
    "                    err = np.dot(err,self.model[iw+1][0].T) * self.relu(layer_zValue,True)\n",
    "\n",
    "                delta[iw][0] += np.dot(layer_xInput.T,err)\n",
    "                delta[iw][1] += err\n",
    "                \n",
    "        return np.sum(cost)*(1/(2*len(xtrain))),[[d[0]/len(xtrain),d[1]/len(xtrain)] for d in delta]\n",
    "    \n",
    "    def train(self,xtrain,ytrain):\n",
    "        batchValues = self.predict_batch(xtrain)\n",
    "        \n",
    "        cost,delta = self.backprop(batchValues,xtrain,ytrain)\n",
    "        \n",
    "        # updating the weights and bias\n",
    "        for i in range(len(self.model)):\n",
    "            self.model[i][0] += -self.learning_rate * delta[i][0]\n",
    "            self.model[i][1] += -self.learning_rate * delta[i][1]\n",
    "            \n",
    "        self.learning_rate *= self.learning_rate_decay   \n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def accuracy(self,predictions,targets):\n",
    "        acc = 0\n",
    "        for pred,ans in zip(predictions,targets):\n",
    "            if np.argmax(ans) == np.argmax(pred[0]):\n",
    "                acc += 1\n",
    "                \n",
    "        return round((100*acc)/len(predictions),3)\n",
    "    \n",
    "    def mean_squared_error(self,prediction,answer,deriv=False):   \n",
    "        if deriv:\n",
    "            return 2*(prediction-answer)\n",
    "        return (answer-prediction)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a neural network with 2 hidden layers and 15 neurons \n"
     ]
    }
   ],
   "source": [
    "model = Neural_Network([4,8,nClasses],xtrain.shape[-1],0.041,0.999,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Cost: 0.6061718364995056 | Train acc: 27.619% | Test acc: 46.667% | LR: 0.04096\n",
      "Epoch: 100 | Cost: 0.3518905371835179 | Train acc: 82.857% | Test acc: 86.667% | LR: 0.03706\n",
      "Epoch: 200 | Cost: 0.20456736603187667 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.03353\n",
      "Epoch: 300 | Cost: 0.15399151994030977 | Train acc: 92.381% | Test acc: 95.556% | LR: 0.03034\n",
      "Epoch: 400 | Cost: 0.1292211169323389 | Train acc: 94.286% | Test acc: 93.333% | LR: 0.02745\n",
      "Epoch: 500 | Cost: 0.11412728193280587 | Train acc: 95.238% | Test acc: 95.556% | LR: 0.02484\n",
      "Epoch: 600 | Cost: 0.10351758783986031 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.02247\n",
      "Epoch: 700 | Cost: 0.09544064882251124 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.02033\n",
      "Epoch: 800 | Cost: 0.08917082224258065 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0184\n",
      "Epoch: 900 | Cost: 0.0841658773245988 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.01665\n",
      "Epoch: 1000 | Cost: 0.08012589986081106 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.01506\n",
      "Epoch: 1100 | Cost: 0.07684458435323274 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.01363\n",
      "Epoch: 1200 | Cost: 0.07412426122292118 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.01233\n",
      "Epoch: 1300 | Cost: 0.07183362255121156 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.01116\n",
      "Epoch: 1400 | Cost: 0.06988671352734399 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.01009\n",
      "Epoch: 1500 | Cost: 0.06821931354052707 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.00913\n",
      "Epoch: 1600 | Cost: 0.06678242799971967 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.00826\n",
      "Epoch: 1700 | Cost: 0.06553749109182315 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00748\n",
      "Epoch: 1800 | Cost: 0.06445361819417333 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00676\n",
      "Epoch: 1900 | Cost: 0.06350592646896747 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00612\n",
      "Epoch: 2000 | Cost: 0.06267445288739429 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00554\n",
      "Epoch: 2100 | Cost: 0.06194672051997984 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00501\n",
      "Epoch: 2200 | Cost: 0.06130919513307398 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00453\n",
      "Epoch: 2300 | Cost: 0.06074801779083907 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0041\n",
      "Epoch: 2400 | Cost: 0.060251580188136124 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00371\n",
      "Epoch: 2500 | Cost: 0.059810634053820595 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00336\n",
      "Epoch: 2600 | Cost: 0.05941799056513261 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00304\n",
      "Epoch: 2700 | Cost: 0.0590676671887463 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00275\n",
      "Epoch: 2800 | Cost: 0.05875461326322117 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00249\n",
      "Epoch: 2900 | Cost: 0.05847448877809037 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00225\n",
      "Epoch: 3000 | Cost: 0.05822354332491875 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00204\n",
      "Epoch: 3100 | Cost: 0.05799850544200809 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00184\n",
      "Epoch: 3200 | Cost: 0.05779652775334857 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00167\n",
      "Epoch: 3300 | Cost: 0.05761509280181779 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00151\n",
      "Epoch: 3400 | Cost: 0.057451998547382 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00136\n",
      "Epoch: 3500 | Cost: 0.05730529736407255 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00123\n",
      "Epoch: 3600 | Cost: 0.05717326341223405 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00112\n",
      "Epoch: 3700 | Cost: 0.05705436941983662 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00101\n",
      "Epoch: 3800 | Cost: 0.056947255739830754 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00091\n",
      "Epoch: 3900 | Cost: 0.056850717841935204 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00083\n",
      "Epoch: 4000 | Cost: 0.05676367535145745 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00075\n",
      "Epoch: 4100 | Cost: 0.05668517060397862 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00068\n",
      "Epoch: 4200 | Cost: 0.05661434161872613 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00061\n",
      "Epoch: 4300 | Cost: 0.05655042264564644 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00055\n",
      "Epoch: 4400 | Cost: 0.05649272308593701 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0005\n",
      "Epoch: 4500 | Cost: 0.05644062771786809 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00045\n",
      "Epoch: 4600 | Cost: 0.056393581369694995 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00041\n",
      "Epoch: 4700 | Cost: 0.05635108784187455 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00037\n",
      "Epoch: 4800 | Cost: 0.05631269958187733 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00034\n",
      "Epoch: 4900 | Cost: 0.05627801510074322 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0003\n",
      "Epoch: 5000 | Cost: 0.05624667278800801 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00028\n",
      "Epoch: 5100 | Cost: 0.05621834692958452 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00025\n",
      "Epoch: 5200 | Cost: 0.05619274482236121 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00023\n",
      "Epoch: 5300 | Cost: 0.05616960164392476 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0002\n",
      "Epoch: 5400 | Cost: 0.056148680096343204 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00018\n",
      "Epoch: 5500 | Cost: 0.0561297645061913 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00017\n",
      "Epoch: 5600 | Cost: 0.056112662239722465 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00015\n",
      "Epoch: 5700 | Cost: 0.056097197506332706 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00014\n",
      "Epoch: 5800 | Cost: 0.056083213430039754 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00012\n",
      "Epoch: 5900 | Cost: 0.0560705670261304 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00011\n",
      "Epoch: 6000 | Cost: 0.05605913011928628 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0001\n",
      "Epoch: 6100 | Cost: 0.05604878640060772 | Train acc: 95.238% | Test acc: 100.0% | LR: 9e-05\n",
      "Epoch: 6200 | Cost: 0.05603943106999119 | Train acc: 95.238% | Test acc: 100.0% | LR: 8e-05\n",
      "Epoch: 6300 | Cost: 0.05603096938091365 | Train acc: 95.238% | Test acc: 100.0% | LR: 7e-05\n",
      "Epoch: 6400 | Cost: 0.0560233155855856 | Train acc: 95.238% | Test acc: 100.0% | LR: 7e-05\n",
      "Epoch: 6500 | Cost: 0.05601639258552061 | Train acc: 95.238% | Test acc: 100.0% | LR: 6e-05\n",
      "Epoch: 6600 | Cost: 0.056010130146951435 | Train acc: 95.238% | Test acc: 100.0% | LR: 6e-05\n",
      "Epoch: 6700 | Cost: 0.05600446540740796 | Train acc: 95.238% | Test acc: 100.0% | LR: 5e-05\n",
      "Epoch: 6800 | Cost: 0.05599934093122751 | Train acc: 95.238% | Test acc: 100.0% | LR: 5e-05\n",
      "Epoch: 6900 | Cost: 0.055994705310510914 | Train acc: 95.238% | Test acc: 100.0% | LR: 4e-05\n",
      "Epoch: 7000 | Cost: 0.05599051170251725 | Train acc: 95.238% | Test acc: 100.0% | LR: 4e-05\n",
      "Epoch: 7100 | Cost: 0.055986717965723506 | Train acc: 95.238% | Test acc: 100.0% | LR: 3e-05\n",
      "Epoch: 7200 | Cost: 0.05598328592376598 | Train acc: 95.238% | Test acc: 100.0% | LR: 3e-05\n",
      "Epoch: 7300 | Cost: 0.05598018099490093 | Train acc: 95.238% | Test acc: 100.0% | LR: 3e-05\n",
      "Epoch: 7400 | Cost: 0.05597737203923789 | Train acc: 95.238% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7500 | Cost: 0.055974830723278045 | Train acc: 95.238% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7600 | Cost: 0.05597253164392933 | Train acc: 95.238% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7700 | Cost: 0.055970451569951216 | Train acc: 95.238% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7800 | Cost: 0.0559685697220571 | Train acc: 95.238% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7900 | Cost: 0.055966867133582336 | Train acc: 95.238% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 8000 | Cost: 0.0559653267538174 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8100 | Cost: 0.0559639331107126 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8200 | Cost: 0.05596267221492262 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8300 | Cost: 0.05596153142788378 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8400 | Cost: 0.05596049927417575 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8500 | Cost: 0.055959565452173156 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8600 | Cost: 0.055958720541415534 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8700 | Cost: 0.055957956114635526 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8800 | Cost: 0.055957264475488044 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8900 | Cost: 0.0559566387078803 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 9000 | Cost: 0.05595607253037388 | Train acc: 95.238% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 9100 | Cost: 0.055955560267608256 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9200 | Cost: 0.055955096788934656 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9300 | Cost: 0.055954677435114564 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9400 | Cost: 0.05595429802355843 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9500 | Cost: 0.055953954729798604 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9600 | Cost: 0.05595364413165437 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9700 | Cost: 0.05595336310331892 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9800 | Cost: 0.05595310883648264 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9900 | Cost: 0.05595287877925051 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.0\n"
     ]
    }
   ],
   "source": [
    "cost = 0\n",
    "interval = 100\n",
    "epochs = 10000\n",
    "for e in range(epochs):\n",
    "    cost += model.train(xtrain,ytrain)\n",
    "    if e%interval == 0:\n",
    "        trainPreds = model.predict_batch(xtrain,True)\n",
    "        testPreds = model.predict_batch(xtest,True)\n",
    "        if e != 0:\n",
    "            cost /= interval\n",
    "        print(\"Epoch: {} | Cost: {} | Train acc: {}% | Test acc: {}% | LR: {}\".format(e,cost,model.accuracy(trainPreds,ytrain),model.accuracy(testPreds,ytest),round(model.learning_rate,5)))\n",
    "        cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
