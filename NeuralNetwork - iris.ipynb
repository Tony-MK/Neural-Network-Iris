{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_oneHotLabels(targets,nClasses):\n",
    "    oneHotLabels = np.zeros((len(targets),nClasses))\n",
    "    for i in range(len(targets)):\n",
    "        oneHotLabels[i][targets[i]] = 1.0\n",
    "    return oneHotLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 150\n"
     ]
    }
   ],
   "source": [
    "nClasses = len(iris.target_names)\n",
    "nFeatures = len(iris.feature_names)\n",
    "data,targets = MinMaxScaler().fit_transform(iris.data),to_oneHotLabels(iris.target,nClasses)\n",
    "print(\"Total Samples: %d\"%(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 150 | Testing Samples: 105\n"
     ]
    }
   ],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(data,targets,test_size=0.3,shuffle=True)\n",
    "print(\"Training Samples: {} | Testing Samples: {}\".format(len(data),xtrain.shape[0],xtest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self,params,inputSize,learning_rate=1.0,learning_rate_decay=0.99,relu_alpha=0.0):\n",
    "        self.model = self.create_neuralNetwork(params,inputSize)\n",
    "    \n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.relu_alpha = relu_alpha\n",
    "        \n",
    "    def create_neuralNetwork(self,params,inputSize):\n",
    "        model = []\n",
    "        nNeurons = 0\n",
    "        for param in params:\n",
    "            model.append([np.random.normal(size=(inputSize,param)), np.zeros((1,param))])\n",
    "            inputSize = param\n",
    "        print(\"Created a neural network with %d hidden layers and %d neurons \"%(len(model)-1,sum(params)))\n",
    "        return model\n",
    "    \n",
    "    def predict(self,xInput):\n",
    "        xInput = np.expand_dims(xInput,axis=0)\n",
    "        for weights,bias in self.model[:-1]:\n",
    "            xInput = self.relu(np.dot(xInput,weights) + bias)\n",
    "        return self.sigmoid(np.dot(xInput,self.model[-1][0])+ self.model[-1][1])\n",
    "    \n",
    "    def compute(self,xInput):\n",
    "        # z = weights * xInput + bias or z = w*x + b\n",
    "        zValues = []\n",
    "        \n",
    "        # a is either sigmoid(z) or # relu(z)\n",
    "        aValues = [] # holds activated z values by sigmoid or relu functions\n",
    "        \n",
    "        xInput = np.expand_dims(xInput,axis=0)\n",
    "        for weights,bias in self.model[:-1]:\n",
    "            z = np.dot(xInput,weights) + bias\n",
    "            zValues.append(z)\n",
    "            \n",
    "            a = self.relu(z)\n",
    "            aValues.append(a)\n",
    "            xInput = a\n",
    "            \n",
    "        z = np.dot(xInput,self.model[-1][0])+self.model[-1][1]\n",
    "        zValues.append(z)\n",
    "        aValues.append(self.sigmoid(z))\n",
    "\n",
    "        return aValues,zValues\n",
    "    \n",
    "    def predict_batch(self,batch,final_prediction=False):\n",
    "        \n",
    "        if final_prediction:\n",
    "            predictions = []\n",
    "            for x in batch:\n",
    "                predictions.append(self.predict(x))\n",
    "            return predictions\n",
    "        \n",
    "        else:\n",
    "            batchValues = {\"a\":[],\"z\":[]}\n",
    "            for x in batch:\n",
    "                aValues,zValues = self.compute(x)\n",
    "                batchValues[\"a\"].append(aValues)\n",
    "                batchValues[\"z\"].append(zValues)\n",
    "                \n",
    "            return batchValues\n",
    "            \n",
    "            \n",
    "        \n",
    "    def relu(self,z,deriv=False):\n",
    "        if deriv:\n",
    "            z[z<0] = 0 \n",
    "            z[z>0] = 1\n",
    "            return z\n",
    "        return np.maximum(z,self.relu_alpha)\n",
    "\n",
    "    def sigmoid(self,z,deriv=False):\n",
    "        out = 1/(1+np.exp(-z))\n",
    "        if deriv:\n",
    "            return out*(1-out)\n",
    "        return out\n",
    "    \n",
    "    def backprop(self,batchValues,xtrain,ytrain):\n",
    "         # delta  = dCost / dWeights = dC/dW = dC/dA * dA/dZ * dZ/dW\n",
    "        delta = [[np.zeros(weights.shape),np.zeros(bias.shape)] for weights,bias in self.model]\n",
    "        \n",
    "        cost = 0\n",
    "        for i in range(len(xtrain)):\n",
    "            iw = len(self.model)\n",
    "            err = 0\n",
    "            for _ in range(iw):\n",
    "                iw -= 1\n",
    "                \n",
    "                if iw != 0:\n",
    "                    layer_xInput = batchValues[\"a\"][i][iw-1]\n",
    "                else:\n",
    "                    layer_xInput = xtrain[i].reshape(1,4)\n",
    "                    \n",
    "                layer_zValue = batchValues[\"z\"][i][iw]\n",
    "                layer_aValue = batchValues[\"a\"][i][iw]\n",
    "                \n",
    "                if iw == len(self.model)-1:\n",
    "                    cost += self.mean_squared_error(layer_aValue,ytrain[i])\n",
    "                    err = self.mean_squared_error(layer_aValue,ytrain[i],True)*self.sigmoid(layer_zValue,True)\n",
    "                else:\n",
    "                    err = np.dot(err,self.model[iw+1][0].T) * self.relu(layer_zValue,True)\n",
    "\n",
    "                delta[iw][0] += np.dot(layer_xInput.T,err)\n",
    "                delta[iw][1] += err\n",
    "                \n",
    "        return np.sum(cost)*(1/(2*len(xtrain))),[[d[0]/len(xtrain),d[1]/len(xtrain)] for d in delta]\n",
    "    \n",
    "    def train(self,xtrain,ytrain):\n",
    "        batchValues = self.predict_batch(xtrain)\n",
    "        \n",
    "        cost,delta = self.backprop(batchValues,xtrain,ytrain)\n",
    "        \n",
    "        # updating the weights and bias\n",
    "        for i in range(len(self.model)):\n",
    "            self.model[i][0] += -self.learning_rate * delta[i][0]\n",
    "            self.model[i][1] += -self.learning_rate * delta[i][1]\n",
    "            \n",
    "        self.learning_rate *= self.learning_rate_decay   \n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def accuracy(self,predictions,targets):\n",
    "        acc = 0\n",
    "        for pred,ans in zip(predictions,targets):\n",
    "            if np.argmax(ans) == np.argmax(pred[0]):\n",
    "                acc += 1\n",
    "                \n",
    "        return round((100*acc)/len(predictions),3)\n",
    "    \n",
    "    def mean_squared_error(self,prediction,answer,deriv=False):   \n",
    "        if deriv:\n",
    "            return 2*(prediction-answer)\n",
    "        return (answer-prediction)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a neural network with 1 hidden layers and 67 neurons \n"
     ]
    }
   ],
   "source": [
    "model = Neural_Network([64,nClasses],xtrain.shape[-1],0.1,0.999,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Cost: 0.57350540272797 | Train acc: 27.619% | Test acc: 46.667% | LR: 0.0999\n",
      "Epoch: 100 | Cost: 0.1809410568449421 | Train acc: 94.286% | Test acc: 97.778% | LR: 0.09039\n",
      "Epoch: 200 | Cost: 0.08711602373133814 | Train acc: 98.095% | Test acc: 97.778% | LR: 0.08178\n",
      "Epoch: 300 | Cost: 0.05995893222829654 | Train acc: 98.095% | Test acc: 97.778% | LR: 0.074\n",
      "Epoch: 400 | Cost: 0.05011061903168163 | Train acc: 98.095% | Test acc: 97.778% | LR: 0.06695\n",
      "Epoch: 500 | Cost: 0.04450019210923006 | Train acc: 98.095% | Test acc: 95.556% | LR: 0.06058\n",
      "Epoch: 600 | Cost: 0.04078701182318838 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.05481\n",
      "Epoch: 700 | Cost: 0.03812043142280221 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.04959\n",
      "Epoch: 800 | Cost: 0.03609242492225407 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.04487\n",
      "Epoch: 900 | Cost: 0.03448843724030981 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.0406\n",
      "Epoch: 1000 | Cost: 0.03320695614090571 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.03673\n",
      "Epoch: 1100 | Cost: 0.0321775377947287 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.03324\n",
      "Epoch: 1200 | Cost: 0.03135182349363216 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.03007\n",
      "Epoch: 1300 | Cost: 0.030666525103626493 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.02721\n",
      "Epoch: 1400 | Cost: 0.030094280147024276 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.02462\n",
      "Epoch: 1500 | Cost: 0.02960496177098086 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.02227\n",
      "Epoch: 1600 | Cost: 0.02918330170752175 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.02015\n",
      "Epoch: 1700 | Cost: 0.028818140265520372 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.01823\n",
      "Epoch: 1800 | Cost: 0.028499476897975655 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.0165\n",
      "Epoch: 1900 | Cost: 0.02822004807688446 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.01493\n",
      "Epoch: 2000 | Cost: 0.027974490115694153 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.01351\n",
      "Epoch: 2100 | Cost: 0.027758109343465426 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.01222\n",
      "Epoch: 2200 | Cost: 0.027566852747117697 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.01106\n",
      "Epoch: 2300 | Cost: 0.027398276916140777 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.01\n",
      "Epoch: 2400 | Cost: 0.027249251402517417 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00905\n",
      "Epoch: 2500 | Cost: 0.027116624333150555 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00819\n",
      "Epoch: 2600 | Cost: 0.02699832898147958 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00741\n",
      "Epoch: 2700 | Cost: 0.02689268200368792 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.0067\n",
      "Epoch: 2800 | Cost: 0.026798343848108724 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00607\n",
      "Epoch: 2900 | Cost: 0.02671392665035173 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00549\n",
      "Epoch: 3000 | Cost: 0.026638331276946453 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00497\n",
      "Epoch: 3100 | Cost: 0.0265705277157956 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00449\n",
      "Epoch: 3200 | Cost: 0.026509704988364458 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00407\n",
      "Epoch: 3300 | Cost: 0.026455073917791706 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00368\n",
      "Epoch: 3400 | Cost: 0.026405947894355335 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00333\n",
      "Epoch: 3500 | Cost: 0.026361617426181087 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00301\n",
      "Epoch: 3600 | Cost: 0.026321608648932848 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00272\n",
      "Epoch: 3700 | Cost: 0.026285709639019727 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00247\n",
      "Epoch: 3800 | Cost: 0.02625341207285369 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00223\n",
      "Epoch: 3900 | Cost: 0.026224296964563173 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00202\n",
      "Epoch: 4000 | Cost: 0.026198041335764727 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00183\n",
      "Epoch: 4100 | Cost: 0.026174356718393098 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00165\n",
      "Epoch: 4200 | Cost: 0.026152985381936392 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00149\n",
      "Epoch: 4300 | Cost: 0.026133695753596416 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00135\n",
      "Epoch: 4400 | Cost: 0.026116281390857594 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00122\n",
      "Epoch: 4500 | Cost: 0.026100556616864213 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00111\n",
      "Epoch: 4600 | Cost: 0.026086354480863855 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.001\n",
      "Epoch: 4700 | Cost: 0.026073525692348363 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00091\n",
      "Epoch: 4800 | Cost: 0.026061935299694045 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00082\n",
      "Epoch: 4900 | Cost: 0.026051462437179294 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00074\n",
      "Epoch: 5000 | Cost: 0.02604199820166463 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00067\n",
      "Epoch: 5100 | Cost: 0.026033444269325426 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00061\n",
      "Epoch: 5200 | Cost: 0.02602571237230893 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00055\n",
      "Epoch: 5300 | Cost: 0.026018722999964394 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.0005\n",
      "Epoch: 5400 | Cost: 0.02601240403981002 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00045\n",
      "Epoch: 5500 | Cost: 0.02600669089974033 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00041\n",
      "Epoch: 5600 | Cost: 0.02600152510015421 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00037\n",
      "Epoch: 5700 | Cost: 0.02599685396909798 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00033\n",
      "Epoch: 5800 | Cost: 0.025992629810569424 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.0003\n",
      "Epoch: 5900 | Cost: 0.025988809697637873 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00027\n",
      "Epoch: 6000 | Cost: 0.0259853548031274 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00025\n",
      "Epoch: 6100 | Cost: 0.025982230108430912 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00022\n",
      "Epoch: 6200 | Cost: 0.025979403920672578 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.0002\n",
      "Epoch: 6300 | Cost: 0.025976847658245723 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00018\n",
      "Epoch: 6400 | Cost: 0.0259745354597541 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00017\n",
      "Epoch: 6500 | Cost: 0.025972443948701514 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00015\n",
      "Epoch: 6600 | Cost: 0.025970552027570693 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00014\n",
      "Epoch: 6700 | Cost: 0.02596884060332201 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00012\n",
      "Epoch: 6800 | Cost: 0.025967292432575388 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.00011\n",
      "Epoch: 6900 | Cost: 0.02596589190532558 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.0001\n",
      "Epoch: 7000 | Cost: 0.02596462493140442 | Train acc: 97.143% | Test acc: 95.556% | LR: 9e-05\n",
      "Epoch: 7100 | Cost: 0.025963478747824197 | Train acc: 97.143% | Test acc: 95.556% | LR: 8e-05\n",
      "Epoch: 7200 | Cost: 0.02596244182881294 | Train acc: 97.143% | Test acc: 95.556% | LR: 7e-05\n",
      "Epoch: 7300 | Cost: 0.025961503745529725 | Train acc: 97.143% | Test acc: 95.556% | LR: 7e-05\n",
      "Epoch: 7400 | Cost: 0.025960655070078217 | Train acc: 97.143% | Test acc: 95.556% | LR: 6e-05\n",
      "Epoch: 7500 | Cost: 0.025959887269751222 | Train acc: 97.143% | Test acc: 95.556% | LR: 6e-05\n",
      "Epoch: 7600 | Cost: 0.02595919262825633 | Train acc: 97.143% | Test acc: 95.556% | LR: 5e-05\n",
      "Epoch: 7700 | Cost: 0.025958564175091226 | Train acc: 97.143% | Test acc: 95.556% | LR: 5e-05\n",
      "Epoch: 7800 | Cost: 0.025957995595913302 | Train acc: 97.143% | Test acc: 95.556% | LR: 4e-05\n",
      "Epoch: 7900 | Cost: 0.025957481184778194 | Train acc: 97.143% | Test acc: 95.556% | LR: 4e-05\n",
      "Epoch: 8000 | Cost: 0.025957015776968188 | Train acc: 97.143% | Test acc: 95.556% | LR: 3e-05\n",
      "Epoch: 8100 | Cost: 0.025956594701666797 | Train acc: 97.143% | Test acc: 95.556% | LR: 3e-05\n",
      "Epoch: 8200 | Cost: 0.025956213736271404 | Train acc: 97.143% | Test acc: 95.556% | LR: 3e-05\n",
      "Epoch: 8300 | Cost: 0.025955869055726152 | Train acc: 97.143% | Test acc: 95.556% | LR: 2e-05\n",
      "Epoch: 8400 | Cost: 0.025955557204628152 | Train acc: 97.143% | Test acc: 95.556% | LR: 2e-05\n",
      "Epoch: 8500 | Cost: 0.025955275053904783 | Train acc: 97.143% | Test acc: 95.556% | LR: 2e-05\n",
      "Epoch: 8600 | Cost: 0.025955019774653313 | Train acc: 97.143% | Test acc: 95.556% | LR: 2e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8700 | Cost: 0.025954788807400108 | Train acc: 97.143% | Test acc: 95.556% | LR: 2e-05\n",
      "Epoch: 8800 | Cost: 0.025954579834722713 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 8900 | Cost: 0.025954390763431676 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9000 | Cost: 0.025954219696581498 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9100 | Cost: 0.02595406491977353 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9200 | Cost: 0.025953924881244886 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9300 | Cost: 0.0259537981772809 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9400 | Cost: 0.025953683538695177 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9500 | Cost: 0.025953579815679473 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9600 | Cost: 0.025953485969211845 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9700 | Cost: 0.025953401058405996 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9800 | Cost: 0.025953324232603266 | Train acc: 97.143% | Test acc: 95.556% | LR: 1e-05\n",
      "Epoch: 9900 | Cost: 0.025953254722018985 | Train acc: 97.143% | Test acc: 95.556% | LR: 0.0\n"
     ]
    }
   ],
   "source": [
    "cost = 0\n",
    "interval = 100\n",
    "\n",
    "for e in range(10000):\n",
    "    cost += model.train(xtrain,ytrain)\n",
    "    if e%interval == 0:\n",
    "        trainPreds = model.predict_batch(xtrain,True)\n",
    "        testPreds = model.predict_batch(xtest,True)\n",
    "        if e != 0:\n",
    "            cost /= interval\n",
    "        print(\"Epoch: {} | Cost: {} | Train acc: {}% | Test acc: {}% | LR: {}\".format(e,cost,model.accuracy(trainPreds,ytrain),model.accuracy(testPreds,ytest),round(model.learning_rate,5)))\n",
    "        cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
