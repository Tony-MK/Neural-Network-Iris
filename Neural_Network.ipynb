{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_oneHotLabels(targets,nClasses):\n",
    "    oneHotLabels = np.zeros((len(targets),nClasses))\n",
    "    for i in range(len(targets)):\n",
    "        oneHotLabels[i][targets[i]] = 1.0\n",
    "    return oneHotLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 150\n"
     ]
    }
   ],
   "source": [
    "nClasses = len(iris.target_names)\n",
    "nFeatures = len(iris.feature_names)\n",
    "data,targets = MinMaxScaler().fit_transform(iris.data),to_oneHotLabels(iris.target,nClasses)\n",
    "print(\"Total Samples: %d\"%(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 150 | Testing Samples: 105\n"
     ]
    }
   ],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(data,targets,test_size=0.3,shuffle=True)\n",
    "print(\"Training Samples: {} | Testing Samples: {}\".format(len(data),xtrain.shape[0],xtest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self,params,inputSize,learning_rate=1.0,learning_rate_decay=0.99,relu_alpha=0.0):\n",
    "        self.model = self.create_neuralNetwork(params,inputSize)\n",
    "    \n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.relu_alpha = relu_alpha\n",
    "        \n",
    "    def create_neuralNetwork(self,params,inputSize):\n",
    "        layers = []\n",
    "        nNeurons = 0\n",
    "        for param in params:\n",
    "            layers.append([np.random.normal(size=(inputSize,param)), np.zeros((1,param))])\n",
    "            inputSize = param\n",
    "        print(\"Created a neural network with %d hidden layers and %d neurons \"%(len(layers)-1,sum(params)))\n",
    "        return layers\n",
    "    \n",
    "    def predict(self,xInput):\n",
    "        xInput = np.expand_dims(xInput,axis=0)\n",
    "        for weights,bias in self.model[:-1]:\n",
    "            xInput = self.relu(np.dot(xInput,weights) + bias)\n",
    "        return self.sigmoid(np.dot(xInput,self.model[-1][0])+ self.model[-1][1])\n",
    "    \n",
    "    def compute(self,xInput):\n",
    "        # z = weights * xInput + bias or z = w*x + b\n",
    "        zValues = []\n",
    "        \n",
    "        # a is either sigmoid(z) or # relu(z)\n",
    "        aValues = [] # holds activated z values by sigmoid or relu functions\n",
    "        \n",
    "        xInput = np.expand_dims(xInput,axis=0)\n",
    "        for weights,bias in self.model[:-1]:\n",
    "            z = np.dot(xInput,weights) + bias\n",
    "            zValues.append(z)\n",
    "            \n",
    "            a = self.relu(z)\n",
    "            aValues.append(a)\n",
    "            xInput = a\n",
    "            \n",
    "        z = np.dot(xInput,self.model[-1][0])+self.model[-1][1]\n",
    "        zValues.append(z)\n",
    "        aValues.append(self.sigmoid(z))\n",
    "\n",
    "        return aValues,zValues\n",
    "    \n",
    "    def predict_batch(self,batch,final_prediction=False):\n",
    "        \n",
    "        if final_prediction:\n",
    "            predictions = []\n",
    "            for x in batch:\n",
    "                predictions.append(self.predict(x))\n",
    "            return predictions\n",
    "        \n",
    "        else:\n",
    "            batchValues = {\"a\":[],\"z\":[]}\n",
    "            for x in batch:\n",
    "                aValues,zValues = self.compute(x)\n",
    "                batchValues[\"a\"].append(aValues)\n",
    "                batchValues[\"z\"].append(zValues)\n",
    "                \n",
    "            return batchValues\n",
    "            \n",
    "            \n",
    "        \n",
    "    def relu(self,z,deriv=False):\n",
    "        if deriv:\n",
    "            z[z<0] = 0 \n",
    "            z[z>0] = 1\n",
    "            return z\n",
    "        return np.maximum(z,self.relu_alpha)\n",
    "\n",
    "    def sigmoid(self,z,deriv=False):\n",
    "        out = 1/(1+np.exp(-z))\n",
    "        if deriv:\n",
    "            return out*(1-out)\n",
    "        return out\n",
    "    \n",
    "    def backprop(self,batchValues,xtrain,ytrain):\n",
    "         # delta  = dCost / dWeights = dC/dW = dC/dA * dA/dZ * dZ/dW\n",
    "        delta = [[np.zeros(weights.shape),np.zeros(bias.shape)] for weights,bias in self.model]\n",
    "        \n",
    "        cost = 0\n",
    "        for i in range(len(xtrain)):\n",
    "            iw = len(self.model)\n",
    "            err = 0\n",
    "            for _ in range(iw):\n",
    "                iw -= 1\n",
    "                \n",
    "                if iw != 0:\n",
    "                    layer_xInput = batchValues[\"a\"][i][iw-1]\n",
    "                else:\n",
    "                    layer_xInput = xtrain[i].reshape(1,4)\n",
    "                    \n",
    "                layer_zValue = batchValues[\"z\"][i][iw]\n",
    "                layer_aValue = batchValues[\"a\"][i][iw]\n",
    "                \n",
    "                if iw == len(self.model)-1:\n",
    "                    cost += self.mean_squared_error(layer_aValue,ytrain[i])\n",
    "                    err = self.mean_squared_error(layer_aValue,ytrain[i],True)*self.sigmoid(layer_zValue,True)\n",
    "                else:\n",
    "                    err = np.dot(err,self.model[iw+1][0].T) * self.relu(layer_zValue,True)\n",
    "\n",
    "                delta[iw][0] += np.dot(layer_xInput.T,err)\n",
    "                delta[iw][1] += err\n",
    "                \n",
    "        return np.sum(cost)*(1/(2*len(xtrain))),[[d[0]/len(xtrain),d[1]/len(xtrain)] for d in delta]\n",
    "    \n",
    "    def train(self,xtrain,ytrain):\n",
    "        batchValues = self.predict_batch(xtrain)\n",
    "        \n",
    "        cost,delta = self.backprop(batchValues,xtrain,ytrain)\n",
    "        \n",
    "        # updating the weights and bias\n",
    "        for i in range(len(self.model)):\n",
    "            self.model[i][0] += -self.learning_rate * delta[i][0]\n",
    "            self.model[i][1] += -self.learning_rate * delta[i][1]\n",
    "            \n",
    "        self.learning_rate *= self.learning_rate_decay   \n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def accuracy(self,predictions,targets):\n",
    "        acc = 0\n",
    "        for pred,ans in zip(predictions,targets):\n",
    "            if np.argmax(ans) == np.argmax(pred[0]):\n",
    "                acc += 1\n",
    "                \n",
    "        return round((100*acc)/len(predictions),3)\n",
    "    \n",
    "    def mean_squared_error(self,prediction,answer,deriv=False):   \n",
    "        if deriv:\n",
    "            return 2*(prediction-answer)\n",
    "        return (answer-prediction)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a neural network with 3 hidden layers and 17 neurons \n"
     ]
    }
   ],
   "source": [
    "model = Neural_Network([4,6,4,nClasses],xtrain.shape[-1],0.041,0.999,0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Cost: 0.510827191197938 | Train acc: 33.333% | Test acc: 8.889% | LR: 0.04096\n",
      "Epoch: 100 | Cost: 0.30219308554549085 | Train acc: 40.952% | Test acc: 35.556% | LR: 0.03706\n",
      "Epoch: 200 | Cost: 0.24954179876244229 | Train acc: 75.238% | Test acc: 84.444% | LR: 0.03353\n",
      "Epoch: 300 | Cost: 0.2140304841622157 | Train acc: 87.619% | Test acc: 97.778% | LR: 0.03034\n",
      "Epoch: 400 | Cost: 0.18567458822794117 | Train acc: 93.333% | Test acc: 97.778% | LR: 0.02745\n",
      "Epoch: 500 | Cost: 0.16319973196656917 | Train acc: 94.286% | Test acc: 100.0% | LR: 0.02484\n",
      "Epoch: 600 | Cost: 0.14529499311218522 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.02247\n",
      "Epoch: 700 | Cost: 0.12875198864190668 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.02033\n",
      "Epoch: 800 | Cost: 0.10845225789628588 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.0184\n",
      "Epoch: 900 | Cost: 0.09823752596959354 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.01665\n",
      "Epoch: 1000 | Cost: 0.09014068164033309 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.01506\n",
      "Epoch: 1100 | Cost: 0.08364826702672659 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.01363\n",
      "Epoch: 1200 | Cost: 0.0783485920969697 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.01233\n",
      "Epoch: 1300 | Cost: 0.0739475965134027 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.01116\n",
      "Epoch: 1400 | Cost: 0.0702803831065273 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.01009\n",
      "Epoch: 1500 | Cost: 0.06719582138559477 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00913\n",
      "Epoch: 1600 | Cost: 0.0645846352736004 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00826\n",
      "Epoch: 1700 | Cost: 0.0623602676429556 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00748\n",
      "Epoch: 1800 | Cost: 0.06045494016565755 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00676\n",
      "Epoch: 1900 | Cost: 0.05881289306903612 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00612\n",
      "Epoch: 2000 | Cost: 0.05739090427839765 | Train acc: 95.238% | Test acc: 100.0% | LR: 0.00554\n",
      "Epoch: 2100 | Cost: 0.05616107524407087 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00501\n",
      "Epoch: 2200 | Cost: 0.05508895572681254 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00453\n",
      "Epoch: 2300 | Cost: 0.05415046815721079 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.0041\n",
      "Epoch: 2400 | Cost: 0.053326409465120114 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00371\n",
      "Epoch: 2500 | Cost: 0.05260023754932894 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00336\n",
      "Epoch: 2600 | Cost: 0.051960027925846176 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00304\n",
      "Epoch: 2700 | Cost: 0.051394178634234594 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00275\n",
      "Epoch: 2800 | Cost: 0.05089189598250858 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00249\n",
      "Epoch: 2900 | Cost: 0.05044540988145637 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00225\n",
      "Epoch: 3000 | Cost: 0.050047857403820284 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00204\n",
      "Epoch: 3100 | Cost: 0.049693322573290545 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00184\n",
      "Epoch: 3200 | Cost: 0.04937664327099285 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00167\n",
      "Epoch: 3300 | Cost: 0.04909345272742764 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00151\n",
      "Epoch: 3400 | Cost: 0.04883995898814974 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00136\n",
      "Epoch: 3500 | Cost: 0.04861279068252402 | Train acc: 96.19% | Test acc: 100.0% | LR: 0.00123\n",
      "Epoch: 3600 | Cost: 0.048409030347017025 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00112\n",
      "Epoch: 3700 | Cost: 0.04822611282924846 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00101\n",
      "Epoch: 3800 | Cost: 0.04806178078946376 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00091\n",
      "Epoch: 3900 | Cost: 0.04791404760703918 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00083\n",
      "Epoch: 4000 | Cost: 0.047781152135237336 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00075\n",
      "Epoch: 4100 | Cost: 0.04766153931235489 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00068\n",
      "Epoch: 4200 | Cost: 0.047553826335345224 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00061\n",
      "Epoch: 4300 | Cost: 0.047456786785877544 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00055\n",
      "Epoch: 4400 | Cost: 0.04736932680967303 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0005\n",
      "Epoch: 4500 | Cost: 0.04729047002326421 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00045\n",
      "Epoch: 4600 | Cost: 0.04721934789489545 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00041\n",
      "Epoch: 4700 | Cost: 0.047155181582683635 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00037\n",
      "Epoch: 4800 | Cost: 0.0470972750595505 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00034\n",
      "Epoch: 4900 | Cost: 0.04704500466417441 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0003\n",
      "Epoch: 5000 | Cost: 0.04699781109875787 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00028\n",
      "Epoch: 5100 | Cost: 0.04695519291530076 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00025\n",
      "Epoch: 5200 | Cost: 0.046916699710594464 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00023\n",
      "Epoch: 5300 | Cost: 0.04688192578666773 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0002\n",
      "Epoch: 5400 | Cost: 0.046850507445878965 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00018\n",
      "Epoch: 5500 | Cost: 0.04682211693789732 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00017\n",
      "Epoch: 5600 | Cost: 0.04679645952361737 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00015\n",
      "Epoch: 5700 | Cost: 0.04677326909846516 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00014\n",
      "Epoch: 5800 | Cost: 0.04675230672954375 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00012\n",
      "Epoch: 5900 | Cost: 0.046733356538291 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.00011\n",
      "Epoch: 6000 | Cost: 0.04671622388302676 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0001\n",
      "Epoch: 6100 | Cost: 0.04670073332705004 | Train acc: 97.143% | Test acc: 100.0% | LR: 9e-05\n",
      "Epoch: 6200 | Cost: 0.04668672657123898 | Train acc: 97.143% | Test acc: 100.0% | LR: 8e-05\n",
      "Epoch: 6300 | Cost: 0.04667406055303538 | Train acc: 97.143% | Test acc: 100.0% | LR: 7e-05\n",
      "Epoch: 6400 | Cost: 0.04666260527333463 | Train acc: 97.143% | Test acc: 100.0% | LR: 7e-05\n",
      "Epoch: 6500 | Cost: 0.046652245249620757 | Train acc: 97.143% | Test acc: 100.0% | LR: 6e-05\n",
      "Epoch: 6600 | Cost: 0.046642875847113505 | Train acc: 97.143% | Test acc: 100.0% | LR: 6e-05\n",
      "Epoch: 6700 | Cost: 0.04663440156643357 | Train acc: 97.143% | Test acc: 100.0% | LR: 5e-05\n",
      "Epoch: 6800 | Cost: 0.04662673683699678 | Train acc: 97.143% | Test acc: 100.0% | LR: 5e-05\n",
      "Epoch: 6900 | Cost: 0.046619804086830356 | Train acc: 97.143% | Test acc: 100.0% | LR: 4e-05\n",
      "Epoch: 7000 | Cost: 0.04661353321958781 | Train acc: 97.143% | Test acc: 100.0% | LR: 4e-05\n",
      "Epoch: 7100 | Cost: 0.04660786088642739 | Train acc: 97.143% | Test acc: 100.0% | LR: 3e-05\n",
      "Epoch: 7200 | Cost: 0.0466027298301893 | Train acc: 97.143% | Test acc: 100.0% | LR: 3e-05\n",
      "Epoch: 7300 | Cost: 0.046598088140589494 | Train acc: 97.143% | Test acc: 100.0% | LR: 3e-05\n",
      "Epoch: 7400 | Cost: 0.0465938893378619 | Train acc: 97.143% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7500 | Cost: 0.04659009084076686 | Train acc: 97.143% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7600 | Cost: 0.04658665460101579 | Train acc: 97.143% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7700 | Cost: 0.046583545954508754 | Train acc: 97.143% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7800 | Cost: 0.04658073361748541 | Train acc: 97.143% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 7900 | Cost: 0.04657818934072775 | Train acc: 97.143% | Test acc: 100.0% | LR: 2e-05\n",
      "Epoch: 8000 | Cost: 0.04657588755418538 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8100 | Cost: 0.046573805103740755 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8200 | Cost: 0.04657192108471619 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8300 | Cost: 0.04657021657480595 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8400 | Cost: 0.04656867445871403 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8500 | Cost: 0.0465672792545139 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8600 | Cost: 0.0465660169545486 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8700 | Cost: 0.0465648748888348 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 8800 | Cost: 0.046563841610789546 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8900 | Cost: 0.046562906752613265 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 9000 | Cost: 0.046562060930892143 | Train acc: 97.143% | Test acc: 100.0% | LR: 1e-05\n",
      "Epoch: 9100 | Cost: 0.046561295665256124 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9200 | Cost: 0.04656060327702059 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9300 | Cost: 0.04655997683409129 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9400 | Cost: 0.046559410046297725 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9500 | Cost: 0.04655889723157744 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9600 | Cost: 0.04655843324956015 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9700 | Cost: 0.04655801345207315 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9800 | Cost: 0.04655763363046174 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n",
      "Epoch: 9900 | Cost: 0.04655728997423556 | Train acc: 97.143% | Test acc: 100.0% | LR: 0.0\n"
     ]
    }
   ],
   "source": [
    "cost = 0\n",
    "interval = 100\n",
    "epochs = 10000\n",
    "for e in range(epochs):\n",
    "    cost += model.train(xtrain,ytrain)\n",
    "    if e%interval == 0:\n",
    "        trainPreds = model.predict_batch(xtrain,True)\n",
    "        testPreds = model.predict_batch(xtest,True)\n",
    "        if e != 0:\n",
    "            cost /= interval\n",
    "        print(\"Epoch: {} | Cost: {} | Train acc: {}% | Test acc: {}% | LR: {}\".format(e,cost,model.accuracy(trainPreds,ytrain),model.accuracy(testPreds,ytest),round(model.learning_rate,5)))\n",
    "        cost = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
